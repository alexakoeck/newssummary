# -*- coding: utf-8 -*-
"""ToolsFile (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G0B2TFGmFD8KnQZzfoYd7sxunDngUduO
"""
import boto3
from boto3.dynamodb.conditions import Attr
import json
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import warnings
warnings.simplefilter(action='ignore',category=FutureWarning)
from langchain_google_genai import GoogleGenerativeAI
from langchain.agents import initialize_agent, Tool

from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate
from langchain_community.tools import BraveSearch
from langchain.agents import initialize_agent, Tool
import ast
import re
import requests
from newspaper import Article
from langchain_community.utilities import GoogleSerperAPIWrapper
import os
from langdetect import detect
REGIONAL_SITES = {
    "GB": ["bbc.com", "theguardian.com"],
    "DE": ["spiegel.de", "zeit.de", "tagesschau.de"],
    "KR": ["yonhapnews.co.kr", "koreatimes.co.kr"]
}

LANG_TO_REGION = {
    "en": "GB",
    "de": "DE",
    "ko": "KR"
}







#region choice and then in the end translate based on region choice
def detect_language_region(text):
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    try:
        prompt=f"What language is used in this prompt? reply with only a 2-letter lowecase ISO 639-1 language code (like 'en', 'de', 'ko').\n\nQuery: {text}"
        response= llm.invoke(prompt)
        lang = code = response.strip().lower() if isinstance(response, str) else response.content.strip().lower()
        #print(lang)
    except:
        lang = "en"
    if lang in LANG_TO_REGION:
        return LANG_TO_REGION[lang]
    else:
        return "GB"

def detect_topic_region_llm(query):
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    prompt = f"What country is this query about? Reply with only the 2-letter ISO country code.\n\nQuery: {query}"
    response = llm.invoke(prompt)
    code = response.strip().upper() if isinstance(response, str) else response.content.strip().upper()
    return code #if code in REGIONAL_SITES else None

def get_news_sources(query):
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    lang_region = detect_language_region(query)
    topic_region = detect_topic_region_llm(query)
    #print(f"Language region: {lang_region}, Topic region: {topic_region or 'Unknown'}")

    region = topic_region if topic_region else lang_region
    if region in REGIONAL_SITES:
        sites= REGIONAL_SITES[region]
        websites = ", ".join(sites)
        return websites
    else:
        websites = None
        return region
  

##if websites is empty search web for reliabel websites make more flexible
def search_web(region):
    ##brave search
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    
    api_key = "BSAvATtaHe21yNPssGoIw8tRKGzBhI9"
    search = BraveSearch.from_api_key(api_key=api_key, search_kwargs={"count": 3})

    tools=[Tool(
        name="BraveSearch",
        func=search.run,
        description="Useful for finding which news organizations are considered reliable in a country."
    )]

    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent="zero-shot-react-description",
        verbose=False)

    response = agent.invoke(f"List the top 3 to 4 most reliable and unbiased news outlets in {region} in any language and give their URLs in form of a python list.")
    output= response['output']
    ##extract the website URLs in a LIst that can be passed to kendra as the predefined ones

    match = re.search(r"```python\n(.*?)\n```", output, re.DOTALL)
    if match:
        code_block = match.group(1)
        # Remove comments (everything after # on each line)
        cleaned = "\n".join([line.split("#")[0].strip() for line in code_block.splitlines()])
        websites = ast.literal_eval(cleaned)
        return websites
    else:
        return output

#def extract summaries

# amazon Kendra not sufficient for real time search so use RAG agent from langchain or haystack
def search_articles(input): #(sites, prompt,prompt_lang, table, bucket_name):
    print(input)
    list = list(input.keys())
    print(list)
    
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    
    #1 extract keywords from prompt ##avoid extra ebmedding and intensive retrival
    phrases= comprehend.detect_key_phrases(Text=prompt, LanguageCode=prompt_lang) ##or 'en'
    keys=[phrase['Text'] for phrase in phrases['KeyPhrases']]

    #2 extract s3 keys with same keywords
        #step1: create a filter for or condition on all key phrases
    filter_expression = Attr('Keywords').contains(keys[0])
    for kw in keys[1:]:
        filter_expression = filter_expression | Attr('Keywords').contains(kw)

        # Step 2: Scan the table
    response = table.scan(FilterExpression=filter_expression)
    items = response['Items']

    # Step 3: Filter items that match at least 3 keywords
    min_items = [item for item in items if len(set(item['Keywords']) & set(search_keywords)) >= 3]

        # Output the S3 keys
    s3_keys=[]
    for item in filtered_items:
        s3_keys.append(item['S3Key'])

    #3 extract texts form these files
    s3_summs = []

    for key in s3_keys:
        response = s3.get_object(Bucket=bucket_name, Key=key)
        content = response['Body'].read()
        json_data = json.loads(content)

    summary = json_data.get('summary')
    if summary:
        s3_summs.append(summary)

    #4 websearch for 3-5 more articles with semantic similarity websearch

    keywords=keys
    api_key = "BSAvATtaHe21yNPssGoIw8tRKGzBhI9"
    tool = BraveSearch.from_api_key(api_key=api_key, search_kwargs={"count": 3})

    def extract_article_body(url):
        try:
            article = Article(url)
            article.download()
            article.parse()
            return {
                "url": url,
                "title": article.title,
                "text": article.text,
                "publish_date": article.publish_date
            }

        except Exception as e:
            print(f"Failed to extract article from {url}: {e}")
            return None

    tools=[Tool(
        name="BraveSearchArticles",
        func=tool.run,
        description="Useful for finding good news articles from predefined websites only."
        )]

    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent="zero-shot-react-description",
        verbose=False)

    response = agent.invoke(f"List most matching general and specific and recent (not older than 6 months)news articles from and only from{sites} matching {prompt} as good as possible\n if no matches for teh prompt try these keywords{keywords}")
    output= response['output']


    #5 extract full text
    web_articles=[]
    match = re.search(r"```python\n(.*?)\n```", output, re.DOTALL)
    if match:
        code_block = match.group(1)
    # Remove comments (everything after # on each line)
        cleaned = "\n".join([line.split("#")[0].strip() for line in code_block.splitlines()])
        websites = ast.literal_eval(cleaned)

    for url in websites:
        doc=extract_article_body(url)
        web_articles.append(doc['text'])

    #6 merge two lists ##if time only select 5 pest out of all only if list length is over 7
    articles= s3_summs+web_articles

    return articles ## should be in form of list of strings

#a tool to translate each article for easier summarization  choosing most overlapping language response can be done by llm
def translate_articles(articles, new_language):
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    
    trans_articles=[]

    for article in articles:
        try:
            prompt=f"What language is used in this prompt? reply with only a 2-letter lowecase ISO 639-1 language code (like 'en', 'de', 'ko').\n\nQuery: {text}"
            response= llm.invoke(prompt)
            article_lang = code = response.strip().lower() if isinstance(response, str) else response.content.strip().lower()
        except:
            article_lang = "en"

        trans_art= translate.translate_text(Text=article, SourceLanguageCode=article_lang,TragetLanguageCode=new_lang)
        fin_rep=trans_art['TranslatedText']
        trans_articles.append(fin_rep)

    return trans_articles

#a tool to translate prompt for better RAG
def translate_prompt(prompt,prompt_lang,new_lang):

    trans_prompt= translate.translate_text(Text=prompt, SourceLanguageCode=prompt_lang,TragetLanguageCode=new_lang)
    prompts=trans_prompt['TranslatedText']

    return prompt

#summarize
def merge(articles):

    llm_sum = GoogleGenerativeAI(model="gemini-2.0-flash",
                             google_api_key= "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A",
                             temperature=0)

    chain = load_summarize_chain(llm_sum, chain_type="map_reduce")
    summary = chain.run(articles)

    return summary ##maybe in eng

#only push to S3 if different enough to retrieved articles
##follow the assumption that the language of response i more likely to be close to articles searched next time than having all in english
def push_to_S3(json_file, topic):

    bucket_name='newssummariesagentprojectdl'
    object_key= f'{topic}.json'
    content=json_file

    s3.put_object(Bucket=bucket_name, Key=object_key, Body=content)
    return

# image generation add thumbnail if it fits text
# def thumbnailgen(response_eng):
    #return image
