# -*- coding: utf-8 -*-
"""ToolsFile (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G0B2TFGmFD8KnQZzfoYd7sxunDngUduO
"""
import boto3
from boto3.dynamodb.conditions import Attr
from langchain_core.documents import Document
import json
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import warnings
warnings.simplefilter(action='ignore',category=FutureWarning)
from langchain_google_genai import GoogleGenerativeAI
from langchain.agents import initialize_agent, Tool
from langchain.chains.summarize import load_summarize_chain
from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate
from langchain_community.tools import BraveSearch
from langchain.agents import initialize_agent, Tool
import ast
import re
import requests
from newspaper import Article
from langchain_community.utilities import GoogleSerperAPIWrapper
import os
import time

from langdetect import detect
REGIONAL_SITES = {
    "GB": ["bbc.com", "theguardian.com"],
    "DE": ["sueddeutsche.de", "zeit.de", "tagesschau.de"],
    "KR": ["yonhapnews.co.kr", "koreatimes.co.kr"]
}

LANG_TO_REGION = {
    "en": "GB",
    "de": "DE",
    "ko": "KR"
}


translate = boto3.client('translate', region_name='us-west-2')
s3= boto3.client('s3', region_name='us-west-2')
comprehend = boto3.client('comprehend',  region_name='us-west-2')


#region choice and then in the end translate based on region choice
def detect_language_region(text):
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    try:
        prompt=f"What language is used in this prompt? reply with only a 2-letter lowecase ISO 639-1 language code (like 'en', 'de', 'ko').\n\nQuery: {text}"
        response= llm.invoke(prompt)
        lang = code = response.strip().lower() if isinstance(response, str) else response.content.strip().lower()
        #print(lang)
    except:
        lang = "en"
    if lang in LANG_TO_REGION:
        return LANG_TO_REGION[lang]
    else:
        return "GB"

def detect_topic_region_llm(query):
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    prompt = f"What country is this query about? Reply with only the 2-letter ISO country code.\n\nQuery: {query}"
    response = llm.invoke(prompt)
    code = response.strip().upper() if isinstance(response, str) else response.content.strip().upper()
    return code #if code in REGIONAL_SITES else None

def get_news_sources(query):
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    lang_region = detect_language_region(query)
    topic_region = detect_topic_region_llm(query)
    #print(f"Language region: {lang_region}, Topic region: {topic_region or 'Unknown'}")

    region = topic_region if topic_region else lang_region
    if region in REGIONAL_SITES:
        websites= REGIONAL_SITES[region]
        #websites = ", ".join(sites)
        return websites
    else:
        websites = None
        return region
  

##if websites is empty search web for reliabel websites make more flexible
def search_web(region):
    time.sleep(10) 
    ##brave search
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    
    api_key = "BSAvATtaHe21yNPssGoIw8tRKGzBhI9"
    search = BraveSearch.from_api_key(api_key=api_key)

    tools=[Tool(
        name="BraveSearch",
        func=search.run,
        description=" need careful input string and is Useful for finding which news websites are considered reliable in a country."
    )]

    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent="zero-shot-react-description",
        handle_parsing_errors=True,
        verbose=False)

    response = agent.invoke(f"List at least 1 and at most 4 trusted news outlets in {region} in any language and give their URLs in form of a python list with maximal length 10.")
    output= response['output']
    ##extract the website URLs in a LIst that can be passed to kendra as the predefined ones

    match = re.search(r"```python\n(.*?)\n```", output, re.DOTALL)
    if match:
        code_block = match.group(1)
        # Remove comments (everything after # on each line)
        cleaned = "\n".join([line.split("#")[0].strip() for line in code_block.splitlines()])
        websites = ast.literal_eval(cleaned)
        return websites
    else:
        return 'try again with broader prompt'

#def extract summaries

# amazon Kendra not sufficient for real time search so use RAG agent from langchain or haystack
def search_articles(input): #(sites, prompt,prompt_lang, bucket_name):
    dynamodb= boto3.resource('dynamodb', region_name='us-west-2')
    table_name='Keywords'
    table=dynamodb.Table(table_name)
    s3= boto3.client('s3', region_name='us-west-2')
    bucket_name='newssummariesagentprojectdl'
    
    import ast
    s = f"{input}"
    data = ast.literal_eval(s)
    keys=list(data.keys())
    sites = data.get(keys[0], )
    if type(sites) != list:
        sites=sites.split(',')
    prompt=data.get(keys[1], )
    prompt_lang=data.get(keys[2],)

   # print(sites)

    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    keywo = prompt.split()
    #1 extract keywords from prompt ##avoid extra ebmedding and intensive retrival 
    #phrases= comprehend.detect_key_phrases(Text=prompt, LanguageCode=prompt_lang) ##or 'en'
    #keys=[phrase['Text'] for phrase in phrases['KeyPhrases']]
    
    #2 extract s3 keys with same keywords
        #step1: create a filter for or condition on all key phrases
    filter_expression = keywo[0]
    for kw in keywo[1:]:
        filter_expression = filter_expression or kw

        # Step 2: Scan the table
    response = table.scan(FilterExpression=Attr('Keywords').contains(filter_expression))
    items = response['Items']

    # Step 3: Filter items that match at least 3 keywords
    min_items = [item for item in items if len(set(item['Keywords']) & set(search_keywords)) >= 2]

        # Output the S3 keys
    s3_keys=[]
    for item in min_items:
        s3_keys.append(item['S3Key'])
    s3_summs = []   
    if len(s3_keys) >= 1:
    #3 extract texts form these files
        

        for key in s3_keys:
            response = s3.get_object(Bucket=bucket_name, Key=key)
            content = response['Body'].read()
            json_data = json.loads(content)

        summary = json_data.get('summary')
        if summary:
            s3_summs.append(summary)

    s3_summs_2=s3_summs[:2]
    #4 websearch for 3-5 more articles with semantic similarity websearch

    keywords=keys
    api_key = "BSAvATtaHe21yNPssGoIw8tRKGzBhI9"
    tool = BraveSearch.from_api_key(api_key=api_key)

    def extract_article_body(url):
        try:
            article = Article(url)
            article.download()
            article.parse()
            return {
                "url": url,
                "title": article.title,
                "text": article.text,
                "publish_date": article.publish_date
            }

        except Exception as e:
            print(f"Failed to extract article from {url}: {e}")
            return None

    tools=[Tool(
        name="BraveSearchArticles",
        func=tool.run,
        description="needs string input! Useful for finding news articles from predefined websites."
        )]

    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent="zero-shot-react-description",
        handle_parsing_error=True,
        verbose=True)

    response = agent.invoke(f"List at least 1 and no more than 7 general or specific news article URL (not older than 6 months)  from{sites} matching {prompt} in form of a python list") #\n if no matches for teh prompt try these keywords{keywords} in form of a python list")
    output= response['output']


    #5 extract full text
    web_articles=[]
    websites=[]
    match = re.search(r"```python\n(.*?)\n```", output, re.DOTALL)
    if match:
        code_block = match.group(1)
    # Remove comments (everything after # on each line)
        cleaned = "\n".join([line.split("#")[0].strip() for line in code_block.splitlines()])
        websites = ast.literal_eval(cleaned)
    if len(websites) >= 1:
        for url in websites[:7]:
            doc=extract_article_body(url)
            if doc['text']:
                web_articles.append(doc['text'])

    #6 merge two lists ##if time only select 5 pest out of all only if list length is over 7
    articles= s3_summs_2+web_articles
    if len(articles) >= 1 : ##reached already
        return articles ## should be in form of list of strings
    else:
        return 'try again with longer prompt or additional websites'
#a tool to translate each article for easier summarization  choosing most overlapping language response can be done by llm
def translate_articles(articles, new_language):
    GOOGLE_API_KEY = "AIzaSyBYT_gvrgceKBEl5-2X5lu5k0s9NS2iV-A"

    llm = GoogleGenerativeAI(
        model="gemini-2.0-flash", ##test gemini-pro maybe better model available in AWS!!
        google_api_key=GOOGLE_API_KEY
    )
    
    trans_articles=[]

    for article in articles:
        try:
            prompt=f"What language is used in this prompt? reply with only a 2-letter lowecase ISO 639-1 language code (like 'en', 'de', 'ko').\n\nQuery: {text}"
            response= llm.invoke(prompt)
            article_lang = code = response.strip().lower() if isinstance(response, str) else response.content.strip().lower()
        except:
            article_lang = "en"

        trans_art= translate.translate_text(Text=article, SourceLanguageCode=article_lang,TragetLanguageCode=new_lang)
        fin_rep=trans_art['TranslatedText']
        trans_articles.append(fin_rep)

    return trans_articles

#a tool to translate prompt for better RAG
def translate_prompt(prompt,prompt_lang,new_lang):

    trans_prompt= translate.translate_text(Text=prompt, SourceLanguageCode=prompt_lang,TragetLanguageCode=new_lang)
    prompts=trans_prompt['TranslatedText']

    return prompt

#summarize
def merge(articles): 
    #time.sleep(60) 
# If you have a list of strings:
    text_list = articles[:5]
    
# Convert to Document objects:
    docs = [Document(page_content=text) for text in text_list]

    llm_sum = GoogleGenerativeAI(model="gemini-2.0-flash",
                             google_api_key= "AIzaSyAQXXXGa-u5bYIcEr09aNXytaJNan1IIWA",
                             temperature=0)

    chain = load_summarize_chain(llm_sum, chain_type="map_reduce")
    summary = chain.invoke(docs)

    return summary ##maybe in eng

#only push to S3 if different enough to retrieved articles
##follow the assumption that the language of response i more likely to be close to articles searched next time than having all in english
def push_to_S3(json_file, topic):

    bucket_name='newssummariesagentprojectdl'
    object_key= topic + '.json'
    content=json_file

    s3.put_object(Bucket=bucket_name, Key=object_key, Body=content)
    return

# image generation add thumbnail if it fits text
# def thumbnailgen(response_eng):
    #return image
